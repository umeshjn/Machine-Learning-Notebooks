{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Text preprocessing is one of the most important first steps in the process of using text for gaining insights. This is one of the most tiring and time consuming as well if we are not using the right tools and techniques. \n",
    "\n",
    "Python has wide range of tools available at our disposal for making this step a very easy to follow process. In this notebook we are going to look at the ways in which we can do tokenization. Tokenization is the process of turning the string into small chunks. These small chunks can be anything from words to sentences. All this depends on your usecase and how you are planning to preprocess the data for your text mining.\n",
    "\n",
    "There are mutliple ways to perform word tokenization. Here we will look at some of the examples.\n",
    "\n",
    "### Basic Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['State',\n",
       " 'delegate',\n",
       " 'equivalents',\n",
       " 'are',\n",
       " 'calculated',\n",
       " 'from',\n",
       " 'the',\n",
       " 'results',\n",
       " 'of',\n",
       " 'the',\n",
       " 'second',\n",
       " 'alignment',\n",
       " 'at',\n",
       " 'each',\n",
       " 'caucus',\n",
       " 'location.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Simplest word tokenization using space to split up the text or document\n",
    "text = \"State delegate equivalents are calculated from the results of the second alignment at each caucus location.\"\n",
    "text.split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK word tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['State',\n",
       " 'delegate',\n",
       " 'equivalents',\n",
       " 'are',\n",
       " 'calculated',\n",
       " 'from',\n",
       " 'the',\n",
       " 'results',\n",
       " 'of',\n",
       " 'the',\n",
       " 'second',\n",
       " 'alignment',\n",
       " 'at',\n",
       " 'each',\n",
       " 'caucus',\n",
       " 'location',\n",
       " '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "### NLTK Word Tokenizer more clean way of tokenizing the text\n",
    "text = \"State delegate equivalents are calculated from the results of the second alignment at each caucus location.\"\n",
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK word tokenizer helps us to tokenize the text and special characters separately which will make the further processer more easier. But this does not work well when we want to tokenize tweets which have hashtags and  we dont want the hashtags to be broken.\n",
    "\n",
    "<b>Example:</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#',\n",
       " 'DelhiElections',\n",
       " 'results',\n",
       " 'will',\n",
       " 'be',\n",
       " 'help',\n",
       " 'this',\n",
       " 'week',\n",
       " 'and',\n",
       " 'results',\n",
       " 'will',\n",
       " 'be',\n",
       " 'next',\n",
       " 'week',\n",
       " '@',\n",
       " 'India']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = \"#DelhiElections results will be help this week and results will be next week @India\"\n",
    "\n",
    "word_tokenize(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not want the hashtags be tokenized into two with '#' seperately. This does not make sense when analyzing the tweets. How do we tokenize the tweets ? NLTK has tokenizer specially built for tweets.\n",
    "\n",
    "### NLTK Tweet tokenizer\n",
    "\n",
    "Tweet tokenizer retains the hashtags and also the tweet handles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#DelhiElections',\n",
       " 'results',\n",
       " 'will',\n",
       " 'be',\n",
       " 'help',\n",
       " 'this',\n",
       " 'week',\n",
       " 'and',\n",
       " 'results',\n",
       " 'will',\n",
       " 'be',\n",
       " 'next',\n",
       " 'week',\n",
       " '@India']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tknz = TweetTokenizer()\n",
    "tknz.tokenize(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing using Regular Expression\n",
    "\n",
    "How do I tokenize and extract only the alpha characters from the tweets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DelhiElections',\n",
       " 'results',\n",
       " 'will',\n",
       " 'be',\n",
       " 'help',\n",
       " 'this',\n",
       " 'week',\n",
       " 'and',\n",
       " 'results',\n",
       " 'will',\n",
       " 'be',\n",
       " 'next',\n",
       " 'week',\n",
       " 'India']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "RegexpTokenizer('[a-zA-Z]\\w+').tokenize(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to extract only the hashtags using regular expression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#DelhiElections']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RegexpTokenizer('^#[a-zA-Z]\\w+').tokenize(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@India']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RegexpTokenizer('@[a-zA-Z]\\w+').tokenize(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence Tokenization\n",
    "\n",
    "sent_tokenize from nltk helps us to tokenize the document into sentences. This is very different from the word tokenize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Processing raw text intelligently is difficult: most words are rare, and it’s common for words that look completely different to mean almost the same thing.',\n",
       " 'The same words in a different order can mean something completely different.',\n",
       " 'Even splitting text into useful word-like units can be difficult in many languages.']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sentences = \"Processing raw text intelligently is difficult: most words are rare, and it’s common for words that look completely different to mean almost the same thing. The same words in a different order can mean something completely different. Even splitting text into useful word-like units can be difficult in many languages.\"\n",
    "sent_tokenize(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Stop words and Special Characters\n",
    "\n",
    "Stop words are nothing but basic words used in most of the sentences in a given language. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Processing',\n",
       " 'raw',\n",
       " 'text',\n",
       " 'intelligently',\n",
       " 'difficult',\n",
       " 'words',\n",
       " 'rare',\n",
       " 'common',\n",
       " 'words',\n",
       " 'look',\n",
       " 'completely',\n",
       " 'different',\n",
       " 'mean',\n",
       " 'almost',\n",
       " 'thing',\n",
       " 'The',\n",
       " 'words',\n",
       " 'different',\n",
       " 'order',\n",
       " 'mean',\n",
       " 'something',\n",
       " 'completely',\n",
       " 'different',\n",
       " 'Even',\n",
       " 'splitting',\n",
       " 'text',\n",
       " 'useful',\n",
       " 'units',\n",
       " 'difficult',\n",
       " 'many',\n",
       " 'languages']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "[w for w in word_tokenize(sentences) if w not in set(stopwords.words('english')) if w.isalpha()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we have seen how we can do tokenization of the text documents. There are multiple ways we can do it. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
